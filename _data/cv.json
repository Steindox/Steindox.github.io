{
  "basics": {
    "name": "Tianlai Jin",
    "email": "steindoxj@gmail.com",
    "phone": "",
    "website": "https://Steindox.github.io",
    "summary": "Undergraduate student at Southern University of Science and Technology (SUSTech). Research focuses on efficient AI, model compression, hardware acceleration, and foundation model optimization. Passionate about working at the intersection of algorithms and hardware. Currently seeking PhD opportunities for Fall 2026.",
    "location": {
      "address": "",
      "postalCode": "",
      "city": "Shenzhen",
      "countryCode": "CN",
      "region": "Guangdong"
    },
    "profiles": [
      {
        "network": "Google Scholar",
        "username": "tianlaiJin",
        "url": "https://scholar.google.com/citations?user=QOcATAYzPg0C&hl=en&oi=ao"
      },
      {
        "network": "GitHub",
        "username": "Steindox",
        "url": "https://github.com/Steindox"
      },
      {
        "network": "LinkedIn",
        "username": "tianlaijin",
        "url": "https://www.linkedin.com/in/tianlaijin-71509332a"
      }
    ]
  },
  "work": [],
  "education": [
    {
      "institution": "Southern University of Science and Technology (SUSTech)",
      "area": "Microelectronics Engineering",
      "studyType": "B.S.",
      "startDate": "2020",
      "endDate": "2026",
      "gpa": null,
      "courses": []
    }
  ],
  "skills": [
    {
      "name": "Programming Languages",
      "level": "",
      "keywords": ["Python", "C++", "CUDA", "PyTorch", "TensorFlow"]
    },
    {
      "name": "Research Areas",
      "level": "",
      "keywords": ["Model Compression", "Hardware Acceleration", "Large Language Models", "Neural Network Pruning", "Foundation Models"]
    },
    {
      "name": "Tools & Frameworks",
      "level": "",
      "keywords": ["Git", "Linux", "Docker", "Hugging Face", "Transformers"]
    }
  ],
  "languages": ["English", "Mandarin"],
  "interests": ["Anime", "Movie", "Guitar"],
  "references": [],
  "publications": [
    {
      "name": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models",
      "publisher": "arXiv preprint",
      "releaseDate": "2024-08-20",
      "website": "https://arxiv.org/abs/2408.10631",
      "summary": "A novel one-shot pruning framework that rebuilds the sparsity mask of pruned models without any retraining or weight reconstruction. Incorporates block-aware error optimization across Self-Attention and MLP blocks, achieving state-of-the-art results in both perplexity and zero-shot performance across various language benchmarks."
    }
  ],
  "presentations": [],
  "teaching": [],
  "portfolio": []
}